name,id,refs,
Pyspark, Pyspark,"Day 1,Day 2,Day 3,Day 4,Day 5,Day 6,Day 7,Day 8",
Day 1,Day 1,"Computing,Service provider,Big Data,HDFS,Map reduce,Cluster,Hadoop,YARN,Batch and Stream Processing,Spark,RDD-",
Day 2,Day 2,"Transformation,Spark Architecture-,Azure,Pyspark-",
Day 3,Day 3,"RDD,Spark Session,Partitions,Variables,Task",
Day 4,Day 4,"Task-,Repartitions,Coalesce,Dataframe,Pyspark vs Pandas Dataframe,StructType,StructField",
Day 5,Day 5,"Array Type in pyspark,Exploding the Data,use of map type,Row and its type,File",
Day 6,Day 6,"Parquet vs ORC File,Acid Transactions,withColumn,Filter,groupby to find the salary department wise",
Day 7,Day 7,"Windows Function,Group by department and state,Checking which one has maximum number in IRIS Dataset,Casting,Modify the column,Reading data from online and applying SQL,Conditioning in the data",
Day 8,Day 8,"Sorting the data,Using union in static data,Using unionbyname in static data,Using union using file,uniononbyname using file,Caching,Persisting,Creating Function,Registering function in UDF,using UDF,Using UDF with more than 1 column",
Sorting the data,Sorting the data,,
Using union in static data,Using union in static data,,
Using unionbyname in static data,Using unionbyname in static data,,
Using union using file,Using union using file,,
uniononbyname using file,uniononbyname using file,,
Caching,Caching,,
Persisting,Persisting,,
Creating Function,Creating Function,,
Registering function in UDF,Registering function in UDF,,
using UDF,using UDF,,
Using UDF with more than 1 column,Using UDF with more than 1 column,,
Windows Function,Windows Function,,
Group by department and state,Group by department and state,,
Checking which one has maximum number in IRIS Dataset,Checking which one has maximum number in IRIS Dataset,,
Casting,Casting,,
Modify the column,Modify the column,,
Reading data from online and applying SQL,Reading data from online and applying SQL,,
Conditioning in the data,Conditioning in the data,,
Parquet vs ORC File,Parquet vs ORC File,,
Acid Transactions,Acid Transactions,,
withColumn,withColumn,"Normal dataset to transform data,Online dataset to transform data",
Filter,Filter,"filter inner column of parent column,to use In using filter,filter if H is starting letter,filter if H Is ending letter,filter if C contains in letter,online dataset for filtering the data",
groupby to find the salary department wise,groupby to find the salary department wise,,
Normal dataset to transform data,Normal dataset to transform data,,
Online dataset to transform data,Online dataset to transform data,,
filter inner column of parent column,filter inner column of parent column,,
to use In using filter,to use In using filter,,
filter if H is starting letter,filter if H is starting letter,,
filter if H Is ending letter,filter if H Is ending letter,,
filter if C contains in letter,filter if C contains in letter,,
online dataset for filtering the data,online dataset for filtering the data,,
Array Type in pyspark,Array Type in pyspark,,
Exploding the Data,Exploding the Data,,
use of map type,use of map type,,
Row and its type,Row and its type,,
File,File,"Avro,Parquet",
Avro,Avro,,
Parquet,Parquet,,
Task-,Task-,"Create RDD from wholeText file,check the partitions of file",
Repartitions,Repartitions,,
Coalesce,Coalesce,,
Dataframe,Dataframe,,
Pyspark vs Pandas Dataframe,Pyspark vs Pandas Dataframe,,
StructType,StructType,,
StructField,StructField,,
Create RDD from wholeText file,Create RDD from wholeText file,,
check the partitions of file,check the partitions of file,,
RDD,RDD,"lineage of RDD,Action Command,DAG and JOB-Stage",
Spark Session,Spark Session,"Creating New Session,Existing Spark Session,Object Name",
Partitions,Partitions,"Default Partitions,Set Partitions",
Variables,Variables,"Broadcast Variable,Accumulator",
Task,Task,"Create RDD with different data type,Get Default Number of Partitions,Create RDD by giving paritions",
lineage of RDD,lineage of RDD,,
Action Command,Action Command,,
DAG and JOB-Stage,DAG and JOB-Stage,,
Creating New Session,Creating New Session,,
Existing Spark Session,Existing Spark Session,,
Object Name,Object Name,,
Default Partitions,Default Partitions,,
Set Partitions,Set Partitions,,
Broadcast Variable,Broadcast Variable,Insert-Update-Display-Remove the data,
Insert-Update-Display-Remove the data,Insert-Update-Display-Remove the data,,
Accumulator,Accumulator,,
Create RDD with different data type,Create RDD with different data type,,
Get Default Number of Partitions,Get Default Number of Partitions,,
Create RDD by giving paritions,Create RDD by giving paritions,,
Transformation,Transformation,"Narrow Transformation,Wide Transformation",
Spark Architecture-,Spark Architecture-,"Spark Driver,Executor,Cluster Manager,Spark Session-,Spark Context & Hive Context & SQL Context",
Azure,Azure,Azure Databricks,
Pyspark-,Pyspark-,,
Narrow Transformation,Narrow Transformation,,
Wide Transformation,Wide Transformation,,
Spark Driver,Spark Driver,,
Executor,Executor,,
Cluster Manager,Cluster Manager,,
Spark Session-,Spark Session-,,
Spark Context & Hive Context & SQL Context,Spark Context & Hive Context & SQL Context,,
Azure Databricks,Azure Databricks,,
Computing,Computing,,
Service provider,Service provider,,
Big Data,Big Data,,
HDFS,HDFS,,
Map reduce,Map reduce,,
Cluster,Cluster,,
Hadoop,Hadoop,,
YARN,YARN,,
Batch and Stream Processing,Batch and Stream Processing,,
Spark,Spark,,
RDD-,RDD-,"Lazy Evaluation,Lineage Graph,Transformation and Action",
Lazy Evaluation,Lazy Evaluation,,
Lineage Graph,Lineage Graph,,
Transformation and Action,Transformation and Action,,
,,,
,,,
,,,
,,,